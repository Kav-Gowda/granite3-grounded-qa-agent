# -*- coding: utf-8 -*-
"""granite3-grounded-qa-agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MeOSqomaxOHTjHwDk1w6wSJW-Weh8eM6
"""

# Commented out IPython magic to ensure Python compatibility.
# # --- Environment Setup ---
# # Install required dependencies
# %%capture
# !pip install langchain==0.2.6 \
#              langchain-community==0.2.6 \
#              ibm-watsonx-ai==1.0.10 \
#              langchain_ibm==0.1.8 \
#              sentence-transformers==3.0.1 \
#              chromadb==0.5.3 \
#              pydantic==2.8.0 \
#              sqlalchemy==2.0.30 \
#              wget==3.2
#

# --- IBM watsonx.ai Client Setup ---

from ibm_watsonx_ai import APIClient, Credentials
import os

credentials = Credentials(
    url="your_watsonx_instance_url"
)

client = APIClient(credentials)
project_id = "your_project_id"

print("IBM watsonx.ai client initialized successfully.")

# --- Load and Prepare Private Documents ---

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# Example placeholder for a private document
# Replace 'data/private_document.txt' with your own secure file path
file_path = "data/private_document.txt"

# Load document locally
loader = TextLoader(file_path)
documents = loader.load()

# Split document into manageable text chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

print(f"Document loaded successfully and split into {len(texts)} chunks.")

# --- Create Embeddings and Build Vector Store ---

from langchain_ibm import WatsonxEmbeddings
from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes
from langchain.vectorstores import Chroma

# Initialize IBM watsonx embeddings (placeholders kept for security)
embeddings = WatsonxEmbeddings(
    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,
    url=credentials["url"],
    project_id=project_id
)

# Build a Chroma vector store from the chunked documents
vector_store = Chroma.from_documents(texts, embeddings)

print("Documents embedded and stored in ChromaDB.")

# --- Define Generation Parameters for Granite 3 ---

from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods

generation_params = {
    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,
    GenParams.MIN_NEW_TOKENS: 1,
    GenParams.MAX_NEW_TOKENS: 100,
    GenParams.STOP_SEQUENCES: ["\n"]
}

print("Granite 3 generation parameters defined.")

# --- Initialize Granite 3 LLM (watsonx.ai) ---

from langchain_ibm import WatsonxLLM

# NOTE: Replace placeholders with your own credentials if running locally.
credentials = {
    "url": "your_watsonx_instance_url"
}

model_id = "ibm/granite-3-8b-instruct"

# Define model parameters
llm_params = {
    "decoding_method": "greedy",
    "temperature": 0.4,
    "min_new_tokens": 1,
    "max_new_tokens": 100
}

project_id = "your_project_id"

# Initialize the watsonx.ai Granite 3 model
granite_llm = WatsonxLLM(
    model_id=model_id,
    url=credentials["url"],
    params=llm_params,
    project_id=project_id
)

print("Granite 3 LLM initialized successfully.")

# --- Build Retrieval-Augmented Generation (RAG) Chain ---

from langchain.chains import RetrievalQA

# Create a retrieval-based Q&A chain
qa_chain = RetrievalQA.from_chain_type(
    llm=granite_llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(),
    return_source_documents=False
)

# Example query for your private document
query = "Summarize the main points discussed in this document."
response = qa_chain.invoke(query)

print(f"ðŸ§  Query: {query}\n")
print("ðŸ’¬ Response:\n", response["result"])